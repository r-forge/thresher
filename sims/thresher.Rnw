\documentclass[11pt]{article}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{hyperref}
\pagestyle{myheadings}
\markright{thresher}

\setlength{\topmargin}{0in}
\setlength{\textheight}{8in}
\setlength{\textwidth}{6.5in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}

\def\rcode#1{\texttt{#1}}
\def\fref#1{\textbf{Figure~\ref{#1}}}
\def\tref#1{\textbf{Table~\ref{#1}}}
\def\sref#1{\textbf{Section~\ref{#1}}}

\title{Simulations for the ``Thresher'' Paper}
\author{Kevin R. Coombes}
\date{29 November 2013}

\SweaveOpts{prefix.string=Figures/thresher}
<<setOptions,echo=FALSE>>=
options(width=88)
options(SweaveHooks = list(fig = function() par(bg='white')))
@ 

<<makeFiguresDirectory,echo=FALSE>>=
if (!file.exists("Figures")) {
  dir.create("Figures")
}
@ 

\begin{document}
\maketitle
\tableofcontents
\listoffigures

\section{Executive Summary}
\subsection{Introduction}
This report describes the (first) analysis of simulated data sets to
test the behavior of our proposed methods for analyzing continuous
pathway data.

\subsubsection{Aims/Objectives}
The primary objective is to create a large number of simulated
datasets so we can see how the methods should work in an ideal
theoretical setting. We also want to explore how well the
outlier-detection method works to separate good ``signal'' proteins
from bad ``noise'' proteins.  Future analyses will look at other
aspects of the method.

\subsection{Methods}

\subsubsection{Description of the Data}
We simulate $2500$ datasets with a few proteins (around $10$--$20$)
and many samples (median: $304$, range: $126$--$506$).  Each dataset
exhibits either one or two independent (i.e., correlated) signals.
Each signal can be unsigned (all proteins are positively correlated,
so a reasonable summary would be a simple average of all proteins) or
signed (including both positively and negatively correlated proteins,
so a reasonable summary requires looking at a difference between two
group averages).  Each dataset also contains two ``noise'' genes that
are not correlated with any of the simulated signals.

\subsubsection{Statistical Methods}
After standardizing (mean zero, standard deviation one) the
measurements from each protein, we perform principal components
analysis (PCA). We use both the Auer-Gervini [2008] Bayesian approach
and the ``broken stick'' model to estimate the number $K$ of
significant components.  For each protein, we then compute its
distance $\Delta$ from the origin in the $K$-dimensional space of PCA
loadings, where $K$ is chosen using the Auer-Gervini approach. We use
histograms and receiver operating characteristic (ROC) curves to
explore how well $\Delta$ separates signal from noise.

\subsection{Results}
\begin{itemize}
\item The broken stick model appears to be somewhat conservative.  It
  never finds more than the true number of components, but in $16\%$
  of the cases with two components, it only finds one of them.
\item The Auer-Gervini approach is more accurate ($94\%$ compared to
  $84\%$) but less conservative (i..e, it finds extra components about
  $0.36\%$ of the time) than the broken-stick model.
\item Failure of the Auer-Gervini method to find the true number of
  components is associated with a small number of proteins
  (\fref{np.estNC}) or a small number of samples
  (\fref{ns.estNC}), but does not depend on the correlation
  (\fref{correl.estNC}).
\item The distance $\Delta$ from the origin is highly effective at
  separating good ``signal'' proteins from bad ``noise'' proteins
  (\fref{good.bad}, \fref{roc}). 
\item Using a cutoff $\Delta > 0.3$ is expected to eliminate $99.6\%$
  of noise proteins while retaining $99.5\$$ of signal proteins
  (\sref{cutoff}).
\end{itemize}

\subsection{Conclusions}
Using the distance from the origin in a principal components loadings
space with dimension determined by the Auer-Gervini Bayesian approach
appears to be an effective way to separate signal from noise.

\section{Preliminaries}
\subsection{Library Packages}
We start by loading all of the R library packages that we need for this
analysis. 
<<libraries>>=
library(Thresher)
library(RColorBrewer)   # for sensible color schemes
@ 
We also define two default color sets.
<<colors>>=
col6 <- c(brewer.pal(5, "Set1"), "#4ea3a3")
col4 <- c("gray", "red", "blue", "purple")
@ 

\subsection{Simulated Data Sets}
With the packages loaded, we can start simulating datasets.  We
simulate five different kinds of datasets. The simulated datasets can
have either one or two true underlying signals, and each signal can
either be all postiively correlated or can include roughly half
positive and half negative correlation. We use the following
procedure:
\begin{enumerate}
\item We select an even number of features (proteins, antibodies,
  genes, etc.) between $10$ and $20$.
\item We split the set of features roughly in half so that we can
  later specify two groups.
\item We also split the features in half to allow for positive and
  negative correlation.
\item We randomly choose a correlation coefficient from a normal
  distribution with mean $0.5$ and standard deviaiton $0.1$.
\item We construct five different correlation/covariance matrices for
  the five kinds of simulated datasets.
\item We then select a number of samples for each dataset from a
  normal distribution with mean $300$ and standard deviation $60$. 
\item Finally, we add two ``noise'' genes to each data set to
  represent uncorrelated outliers.
\end{enumerate}
We repeat this procedure $500$ times, which produces a total of $2500$
simulated datasets.
<<simloop>>=
nSimSets <- 500
f <- "savedSims.rda"
if (file.exists(f)) {
  load (f)
} else {
  set.seed(159708)
  savedSims <- list()
  bsdim <- list()
  counter <- 0
  for (idx in 1:nSimSets) {
    cat(idx, "\n", file=stderr())
# parameters
    nProtein <- 2*sample(5:10,1)
    splinter <- sample((nProtein/2) + (-3:3), 1)
    positive <- sample(nProtein, nProtein/2)
    negative <- (1:nProtein)[!((1:nProtein) %in% positive)]
    posi <- positive[positive <= splinter]
    nega <- negative[negative <= splinter]
# one group, unsigned
    rho <- rnorm(1, 0.5, 0.1)
    sigma1 <- matrix(rho, ncol=nProtein, nrow=nProtein)
    diag(sigma1) <- 1
# two groups, unsigned
    sigma2 <- sigma1
    sigma2[(1+splinter):nProtein, 1:splinter] <- 0
    sigma2[1:splinter, (1+splinter):nProtein] <- 0
# one group, signed
    sigma3 <- sigma1
    sigma3[positive, negative] <- -sigma3[positive, negative]
    sigma3[negative, positive] <- -sigma3[negative, positive]
# two groups, signed
    sigma4 <- sigma2
    sigma4[positive, negative] <- -sigma4[positive, negative]
    sigma4[negative, positive] <- -sigma4[negative, positive]
# two groups, mixed
    sigma5 <- sigma2
    sigma5[posi, nega] <- -sigma5[posi, nega]
    sigma5[nega, posi] <- -sigma5[nega, posi]
# number of samples
    nSample <- round(rnorm(1, 300, 60))
    for (nm in paste("sigma", 1:5, sep='')) {
# add noise
      ss <- matrix(0, nProtein+2, nProtein+2)
      diag(ss) <- 1
      ss[1:nProtein, 1:nProtein] <- get(nm)
# basic analysis
      counter <- counter+1
      value <- SimThresher(ss, nSample, 
                           paste(nm, counter, sep="."),
                           method='auer.gervini')
      savedSims[[counter]] <- value
      bsdim[[counter]] <- bsDimension(value@spca)
    }
  }
  save(savedSims, bsdim, file=f)
}
rm(f)
@ 

\section{Interpreting the Results}
We need to extract various useful pieces of data from the simulation
results.  We start by defining the ``type'' of simulated dataset.
<<simType>>=
simpleType <- paste(rep(c("OneGroup", "TwoGroups"), times=2),
                    rep(c("Unsigned", "Signed"), each=2), sep="")
simpleType <- c(simpleType, "TwoGroupsMixed")
rt <- rep(1:5, nSimSets)
simType <- factor(simpleType[rt], levels=simpleType)
typer <- rep(simType, each=2)
evens <- sort(c(seq(2, 5*nSimSets, 5), 
                seq(4, 5*nSimSets, 5),
                seq(5, 5*nSimSets, 5)))
sim.type <- factor(simType[evens])
rm(rt)
summary(simType)
summary(sim.type)
@ 

\subsection{The broken stick model sometimes finds too few components}
We are trying two different automated methods to estimate the number
of ``statistically significant'' components in each principal
component space: the Auer-Gervini Bayesian approach and the
broken-stick model. Here we look at how well the estimates from the
broken-stick model match the true number of components that we put
into the simulation.
<<bsdim>>=
bsdim <- unlist(bsdim)
table(simType, bsdim)
@ 
We see that the estimate appears to be somewhat conservative.  It
never finds more than the true number of components, but in at least
sixteen percent (80/500) of the cases with two true components, it
only finds one of them.

\subsection{The Auer-Gervini approach is more accurate but less conservative}
Now we look at how well the Auer-Gervini estimates match the true
number of components.
<<estNC>>=
estNComponents <- unlist(lapply(savedSims, function(v) v@pcdim))
table(simType, estNComponents)
@ 
In rare cases (9/2500, about one-third of one percent), the
Auer-Gervini method overestimates the number of components, possibly
because of a weak signal in the presence of the two noise features.
However, it only gets a wrong answer in the presence of two true
components about 6 percent of the time (30/500).

The following table shows that the broken-stick model is almost always
more conservative than the Auer-Gervini approach.
<<compareMethods>>=
table(bsdim, estNComponents)
@ 

\subsection{Understanding conservatism}
There are several possible explanations for the failure of the
Auer-Gervini method to to find the second true component.
\begin{enumerate}
\item It may be harder when the number of samples is small.
\item It may be harder when the true correlation is small.
\item It may be harder when the number of true signal proteins is
  small. 
\end{enumerate}
We can explore each of these potential explanation graphically,
starting with the number of samples (\fref{ns.estNC}).  This figure
suggests that the number of samples may be ver weakly related to
whether or not the algorithm detects the correct number of components.
<<ns>>=
ns <- unlist(lapply(savedSims, function(val) val@nSample))
summary(ns)
@ 
\begin{figure}
<<fig=TRUE,echo=FALSE,width=6, height=8>>=
opar <- par(mfrow=c(2,1))
plot(factor(estNComponents)[evens], ns[evens],
     xlab="Estimated Number of Components",
     ylab="Number of Samples")
interaction.plot(estNComponents[evens], sim.type, ns[evens],
                 xlab="Estimated Number of Components",
                 ylab="Number of Samples")
par(opar)
@ 
\caption{Number of components versus the number of samples;
  \textbf{(top)} box-and-whisker plot, \textbf{(bottom)} interaction
  plot.}
\label{ns.estNC}
\end{figure}

Next, we look at the true correlation coefficients from each
simulation (\fref{correl.estNC}. This figure supports the idea that
when the algorithm should find two components but only finds one, then
the true correlation is likely to be irrelevant.
<<correl>>=
correl <- unlist(lapply(savedSims, function(v) v@rho))
@ 
\begin{figure}
<<fig=TRUE,echo=FALSE,width=6, height=8>>=
opar <- par(mfrow=c(2,1))
plot(factor(estNComponents)[evens], correl[evens],
     xlab="Estimated Number of Components",
     ylab="True Correlation")
interaction.plot(estNComponents[evens], sim.type, correl[evens],
                 xlab="Estimated Number of Components",
                 ylab="Mean True Correlation")
par(opar)
@ 
\caption{Number of components versus the true correlation between
  proteins; \textbf{(top)} box-and-whisker plot, \textbf{(bottom)}
  interaction plot.}
\label{correl.estNC}
\end{figure}

Finally, we look at the effect of the number of true signal proteins
(\fref{np.estNC}). This figure shows that the number of proteins is
likely to be an important predictor of the failure to identify both
components. 
<<np>>=
np <- unlist(lapply(savedSims, function(val) ncol(val@data)))
temp <- rep(1:5,  nSimSets)
classy <- rep(temp, times=np)
@ 
\begin{figure}
<<fig=TRUE,echo=FALSE,width=6, height=8>>=
opar <- par(mfrow=c(2,1))
plot(factor(estNComponents)[evens], np[evens],
     xlab="Estimated Number of Components",
     ylab="Number of Proteins")
interaction.plot(estNComponents[evens], sim.type, np[evens],
                 xlab="Estimated Number of Components",
                 ylab="Number of Proteins")
par(opar)
@ 
\caption{Number of components versus the number of true ``signal''
  proteins; \textbf{(top)} box-and-whisker plot, \textbf{(bottom)}
  interaction plot.}
\label{np.estNC}
\end{figure}

We can use logistic regression to determine whether the number of
proteins and the correlation give independent predictors of the
inability to find both true compnents. First, we try an additive
model: 
<<slayer>>=
slayer <- data.frame(NC=1*(estNComponents > 1), ns, np, correl)[evens,]
model <- glm(NC ~ ns + np + correl, data=slayer, family=binomial)
anova(model, test="Chisq")
@ 
As we expected from the graphical views, the number of proteins is
highly significant, the number of samples is significant, and the
correlation coefficient is irrelevant.  So, we can ignore the
correlation in  what folows.
<<slayer>>=
slayer <- data.frame(NC=1*(estNComponents > 1), ns, np, correl)[evens,]
addmodel <- glm(NC ~ ns + np, data=slayer, family=binomial)
anova(addmodel, test="Chisq")
@ 
In the aditive model that ignores correlation, both terms are
statistically significant, and independent. We now check whether there
is an interaction between the predictive factors.
<<intmodel>>=
intmodel <- glm(NC ~ np*ns, data=slayer, family=binomial)
anova(intmodel, test="Chisq")
@ 
The interaction term is not significant.  So, we record the coefficients
in the additive model:
<<coef>>=
coef(addmodel)
@ 

\subsection{Separating the Wheat from the Chaff}
In the code loop above where we simulated the datasets, we computed
the distance $\Delta$ from the origin (in a Bayesian-defined
$K$-dimensional space) for each protein in the dataset.  These
separate into ``good'' proteins (which are part of the signals we are
trying to detect) and ``bad'' proteins (which are the noise we are
trying to remove.  The next block of code extracts the distances
$\Delta$ for these two sets of proteins.
<<goody>>=
goody <-   unlist(lapply(savedSims, function(val) {
  np <- ncol(val@data)-2
  val@delta[1:np]
}))
@ 

<<baddy>>=
baddy <-   unlist(lapply(savedSims, function(val) {
  np <- ncol(val@data)-2
  val@delta[np+(1:2)]
}))
@ 

<<breakpoints>>=
breakpoints <- seq(0, 1, length=201)
@ 

Plotting histograms of the distances $\Delta$ of the protein features
from the origin suggests that this procedure does a pretty good job of
separating the signal from the noise (\fref{good.bad}).  We also note,
however, the small ``bump'' of noise proteins with very large
coefficients (near 1.0), suggesting that no amount of parameter-tuning
or cutoff-choosing is going to exclude them.
\begin{figure}
<<fig=TRUE,echo=FALSE,width=6,height=8>>=
opar <- par(mfrow=c(2,1))
hist(goody, breaks=breakpoints, main="SIgnal Proteins", 
     xlab="Distance from origin")
hist(baddy, breaks=breakpoints, main="Noise Proteins",
     xlab="Distance from origin")
par(opar)
@ 
\caption{Histograms of the distance from the origin of the ``true''
  signal proteins and the additional noise proteins. }
\label{good.bad}
\end{figure}

We also prepared a plot of the receive operating characteristics (ROC)
curve for using $\Delta$ to ditinguish signal from noise proteins
(\fref{roc}). As suggested above, the separation given by the ROC
curve is excellent.
\begin{figure}
<<fig=TRUE,echo=FALSE>>=
tp <- sapply(breakpoints, function(x) mean(goody > x))
fp <- sapply(breakpoints, function(x) mean(baddy > x))
plot(fp, tp, type='l', lwd=3,
     xlab="False Positive Rate (1 - specificity)",
     ylab="True Positive Rate (sensitivity)")
abline(0,1)
@ 
\caption{Receiver operating characteristic (ROC) curve, based on
  simulated datasets, for the proposed method of separating (true)
  signal from (false) noise among the protein features.}
\label{roc}
\end{figure}

\subsection{Understanding Anti-conservatism}
Here we start by looking at the ``bad = noise'' features that have a
large distance from the origin.
<<bigbad>>=
w <- unique(round(which(baddy > 0.5)/2))
data.frame(EstNC=estNComponents, nSamples=ns, nProteins=np, Correl=correl)[w,]
@ 
We see that there appears to be nothing special about the number of
samples or the number of proteins.  However, this set includes all
nine cases where the number of components is estimated to equal four
(i.e., more than truth).  More importantly, all of the correlation
coefficients are very high (i.e., in the top $3\%$ for this simulation):
<<sumcor>>=
summary(correl)
mean(correl > 0.7)
@ 
This finding suggest that Auer-Gervini is only likely to include the
two noise features as independent significant components if the
features in the true signals are very highly correlated.

\subsubsection{Selecting a Cutoff}
\label{cutoff}
The next task is to try to find a reasonable place to draw a cutoff,
based on $\Delta$, between the good signal proteins and the bad noise
proteins. Of course, this depends on whether we are more concerned
about omitting true signals or about including noise. To begin, we
compute the quantiles of $\Delta$ for the signal proteins.
<<quant>>=
quantile(goody, seq(0.01, 0.05, by=0.01))
@ 
This computation suggests that a reasonable cutoff is certainly less
than $0.6$.
<<testRange>>=
testRange <- seq(0.2, 0.6, by=0.05)
@ 

The next function uses the cutoff on $\Delta$ to compute the true
positive and false positive rates at the corresponding point on the
ROC curve.
<<cutter>>=
cutter <- function(r) {
  w <- 1+sum(breakpoints <= r)
  c(R=r, FP=fp[w], TP=tp[w])
}
@ 
We apply this function at the points in the test range.
<<cutable>>=
cutterTable <- t(matrix(unlist(lapply(testRange, cutter)), nrow=3))
colnames(cutterTable) <- c("Delta", "FP", "TP")
cutterTable
@ 
Well, this helps narrow things down.  We probably don't want to set a
cutoff below $\Delta = 0.25$, since this starts admitting too many
(i.e., at least $1\%$) noise proteins. And we probably don't want to
go above $\Delta=0.35$,  since we keep losing signal proteins without
corresponding gains in reducing the noise proteins.
<<refine>>=
refine <- t(matrix(unlist(lapply(seq(0.25, 0.35, by=0.01), cutter)), nrow=3))
colnames(refine) <- c("Delta", "FP", "TP")
refine
@ 
It seems to me that a cutoff anywhere between $0.30$ and $0.40$ is
reasonable, which gives a false negative rate of about 5 in 1000 and a
false positive rate about 4 in 1000.  Because the peformance is so
consistent across this range, we select the smaller value ($0.3$) as
our default cutoff, since this will eventually retain as many true
positives as possible.

\section{More Detailed Methods}
In this appendix, we document the main functions in the Thresher
package.  

\subsection{Thresher}
The first major step in the analysis is to identify and remove
outliers from the set of antibodies (features; predictors; or pathway
members).  Outliers are features that appear to be simply noise,
without being correlated to anything else in the putative pathway. The
function that carries out this procedure is called \texttt{Thresher},
by analogy with the well-known idiom about separating the wheat from
the chaff.

<<thresher>>=
Thresher
@ 
The algorithm in \texttt{thresher} basically works as follows. 
\begin{enumerate}
\item We start by standardizing the data from each antibody (so it has
  mean zero and standard deviation one).
\item We perform a PCA on the standardized pathway dataset.
\item We apply the Auer-Gervini approach to select the number $K$ of
  statistically significant components.  (See below.)
\item Next, for each antibody, we compute its (Euclidean) distance
  from the origin in the $K$-dimensional space of loadings; this is a
  reflection of the amount of ``influence'' that the antibody has in
  clustering the samples.
\item Antibodies whose influence is smaller than a specfied cutoff
  will be identified as outliers and removed from the dataset.
\end{enumerate}

\subsection{Auer-Gervini} 
The original method described in the Auer-Gervini 2008 paper is a
graphical tool for selecting the number $K$ of components.  We have
developed an additional rule that allows us to automate this
procedure.  We can illustrate how this works using one of our
simulated data sets.
<<s>>=
s <- savedSims[[5]]
@ 
Auer and Gervini put an exponential prior distribution on the number
of principal components.  This prior distribution imposes the
condition that the probability associated with a dimension $K$ is a
non-increasing function of $K$; in other words, more components are
less likely than fewer components.  The prior distribution has a
tunable hyperparameter, $\theta$, that governs how swiftly the prior
probability decreases.  When $\theta=0$, every possible number of
components is equally likely, and in the posterior distribtion, all
components are declared significant.  As $\theta$ increases, the
number of components selected by the posterior distribution decreases
in the form of a step function (\fref{ag}).  (Interestingly, some
values of $K$ can never be chosen by the Bayesian procedure.)  For
large anough values $\theta > \theta_0$, the posterior distribution
will always choose zero components.
\begin{figure}
<<fig=TRUE,echo=FALSE>>=
plot(s@ag)
@ 
\caption{Auer-Gervini step function relating the prior hyperparameter
  $\theta$ to the maximum posterior estimate of the nunber $K$ of
  significant principal components. In this example, our automated
  procedure selects $K=2$.}
\label{ag}
\end{figure}

Auer and Gervini suggest selecting the largest value of $K$ for which
the step-length is ``significant'', since this value of $K$ would be
chosen by the largest number of ``reaonable'' prior values of
$\theta$. One problem with their approach is that ``significant'' is
still subjective.  A second problem is that the step length
corresponding to chossing $K=0$ is actually infinite (since every
value greater than $\theta_0$ is on this step). Our innovations are as
follows:
\begin{enumerate}
\item We compute an upper bound on ``reaonable'' values of $\theta$,
  which corresponds to the value that assigns at most $99\%$ of the
  prior probability to $K=0$, and keeps at least $1\%$ of the prior
  probability on $K \ge 1$.
\item We define ``significant length'' to be at least twice the mean
  length. 
\end{enumerate}

<<adg>>=
Thresher:::estimateTop
agDimension
@ 

\section{Appendix}

This analysis was run in the following directory:
<<getwd>>=
getwd()
@ 
This analysis was run in the following software environment:
<<sessionInfo>>=
sessionInfo()
@ 

\end{document}
